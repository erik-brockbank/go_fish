#labels = comparison_set) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_y_continuous(breaks = seq(1, 7)) +
individ_plot_theme
}
# Bar chart of experiment completion time or avg. trial time
plot_time_data = function(time_summary, ylab, ymax, title) {
time_summary %>%
ggplot(aes(x = Condition, y = mean_task_time,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
ylim(0, ymax) +
labs(x = "", y = ylab) +
ggtitle(title) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank(),
plot.title = element_text(size = 32, face = "bold"))
}
# Bar chart of average memory accuracy across conditions
plot_memory_data = function(memory_summary) {
memory_summary %>%
ggplot(aes(x = Condition, y = mean_memory_accuracy,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
geom_hline(yintercept = 0.5, linetype = "dashed", size = 1) +
ylim(c(0, 1)) +
labs(x = "", y = "Mean memory accuracy") +
# ggtitle("Memory probe accuracy across conditions") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Read in data
summary_data = bind_rows(read_summary_data(ROUND1_SUMMARY_DATA, TRUE),
read_summary_data(ROUND2_SUMMARY_DATA, FALSE))
trial_data = bind_rows(read_trial_data(ROUND_1TRIAL_DATA, TRUE),
read_trial_data(ROUND2_TRIAL_DATA, FALSE))
trial_data = bind_rows(read_trial_data(ROUND1_TRIAL_DATA, TRUE),
read_trial_data(ROUND2_TRIAL_DATA, FALSE))
generation_free_resp_coded = read_coded_free_resp_data(GENERATION_RESP_DATA_CODED, summary_data)
GENERATION_RESP_DATA_CODED = "round2/free_response_combined.csv"
# Read in data
summary_data = bind_rows(read_summary_data(ROUND1_SUMMARY_DATA, TRUE),
read_summary_data(ROUND2_SUMMARY_DATA, FALSE))
trial_data = bind_rows(read_trial_data(ROUND1_TRIAL_DATA, TRUE),
read_trial_data(ROUND2_TRIAL_DATA, FALSE))
generation_free_resp_coded = read_coded_free_resp_data(GENERATION_RESP_DATA_CODED, summary_data)
GENERATION_RESP_DATA_CODED = "free_response_combined.csv"
# Read in data
summary_data = bind_rows(read_summary_data(ROUND1_SUMMARY_DATA, TRUE),
read_summary_data(ROUND2_SUMMARY_DATA, FALSE))
trial_data = bind_rows(read_trial_data(ROUND1_TRIAL_DATA, TRUE),
read_trial_data(ROUND2_TRIAL_DATA, FALSE))
generation_free_resp_coded = read_coded_free_resp_data(GENERATION_RESP_DATA_CODED, summary_data)
generation_judgment_data = bind_rows(read_generation_judgment_data(ROUND1_GENERATION_JUDG_DATA, TRUE),
read_generation_judgment_data(ROUND2_GENERATION_JUDG_DATA, FALSE))
evaluation_data = bind_rows(read_evaluation_data(ROUND1_EVAL_DATA, TRUE),
read_evaluation_data(ROUND2_EVAL_DATA, FALSE))
memory_data = bind_rows(read_memory_data(ROUND1_MEMORY_DATA, TRUE),
read_memory_data(ROUND2_MEMORY_DATA, FALSE))
# Summarize data
prediction_summary = get_prediction_summary(trial_data)
generation_free_resp_summary = get_generation_free_response_summary(generation_free_resp_coded)
generation_judgment_subject_summary = get_generation_judgment_subj_summary(generation_judgment_data)
generation_judgment_summary = get_generation_judgment_summary(generation_judgment_subject_summary)
evaluation_summary = get_evaluation_summary(evaluation_data)
completion_time_summary = get_time_summary(summary_data)
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
trial_time_subject_summary = get_trial_time_subj_summary(trial_data)
trial_time_summary = get_trial_time_summary(trial_time_subject_summary)
memory_subject_summary = get_memory_subj_summary(memory_data)
memory_summary = get_memory_summary(memory_subject_summary)
# 1. Generation free response task
# NB: this plot not included in cog sci submission, but statistics are reported
plot_generation_free_responses(generation_free_resp_summary)
generation_props = generation_free_resp_coded %>%
group_by(Condition) %>%
summarize(success = sum(Revision),
total = n())
chisq_gen = prop.test(c(generation_props$success), c(generation_props$total))
report_chisq_summary(chisq_gen)
# 2. Generation judgment task: overall accuracy across conditions
plot_generation_judgments(generation_judgment_summary)
t_gen = t.test(
generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Describe"],
generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Explain"],
var.equal = T
)
report_t_summary(t_gen)
# 3. Generation judgment task: proportion of people who got 100% across conditions
# NB: similar Chi-sq test with raw count of people who were *coded as getting rule* is what we report above
# for the free response task so this is effectively a parallel analysis for people who got 100% on judgment task
generation_task_comparison = generation_free_resp_coded %>%
inner_join(generation_judgment_subject_summary, by = c("subjID", "Condition"))
# NB: we want table to be subj_accuracy == 1 but we do < 1 below to get count of == 1 as the *first* column
count_data = table(generation_task_comparison$Condition, generation_task_comparison$subj_accuracy < 1)
chisq.test(count_data) # NB: this is equivalent to prop.test(count_data)
# 1. Evaluation of target rule across conditions
plot_evaluation_results(evaluation_summary, RULE_EVAL_LABELS)
t_eval = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe" & evaluation_data$is_target_rule == TRUE],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain" & evaluation_data$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_eval)
# 2. Wilcoxon signed-rank test showing that target rule is different from all other rules across both groups
eval_summary_other_rules = evaluation_data %>%
filter(is_target_rule == FALSE) %>% # for target rule, summarize across participants
group_by(is_target_rule, Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating))
eval_difference = evaluation_data %>%
group_by(subjID, Condition) %>%
filter(is_target_rule == TRUE) %>%
inner_join(., eval_summary_other_rules, by = "subjID") %>%
mutate(diff = input_rule_rating - mean_subj_rating) %>%
select(subjID, Condition.x, diff)
# TODO is this right? Is the conversion from V to z okay?
wil_exp = wilcox.test(eval_difference$diff[eval_difference$Condition.x == "Explain"], exact = F)
wil_des = wilcox.test(eval_difference$diff[eval_difference$Condition.x == "Describe"], exact = F)
report_wilcox_summary(wil_exp) # Explainers
report_wilcox_summary(wil_des) # Describers
# 3.1 Evaluation of distractor rule across conditions
t_eval_comp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == DISTRACTOR_RULE],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_eval_comp)
# 3.2 Comparison of distractor rule and target rule across conditions
t_eval_target_dist_exp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
t_eval_target_dist_desc = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_eval_target_dist_exp) # Explainers
report_t_summary(t_eval_target_dist_desc) # Describers
# Test whether this difference is significant with ANOVA
eval_data_distractor_target = evaluation_data %>%
filter(is_target_rule == T | rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish.")
# sanity check
unique(eval_data_distractor_target$rule_text)
table(eval_data_distractor_target$subjID)
interaction_test = aov(data = eval_data_distractor_target, input_rule_rating ~ Condition*is_target_rule)
summary(interaction_test)
# 4.1 Evaluation of target rule across conditions (this matches 1 above)
subset_participants = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
filter(Revision == 0) %>%
# filter(Revision == 1) %>%
inner_join(., evaluation_data, on = c("subjID", "Condition"))
# Sanity check the join above
table(subset_participants$subjID) # 6 (number of eval rows) for each subj ID
unique(subset_participants$subjID) # 56 for incorrect rule gen, 30 for correct
sum(generation_free_resp_coded$Revision) # 30 for incorrect rule gen: this is equal to 86 (total) - number of unique participants above or equal to number of unique part. above
# Plot data
# NB: this plot not included in cog sci submission
eval_summary_subset = get_evaluation_summary(subset_participants)
plot_evaluation_results(eval_summary_subset, RULE_EVAL_LABELS)
# Analysis: compare ratings of target rule
t_subset_target = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$is_target_rule == TRUE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_subset_target)
# 4.2 Evaluation of distractor rule across conditions (this matches 3.1 above)
t_subset_dist = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == DISTRACTOR_RULE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_subset_dist)
report_t_summary(t_subset_target)
# 4.2 Evaluation of distractor rule across conditions (this matches 3.1 above)
t_subset_dist = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == DISTRACTOR_RULE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_subset_dist)
# 5. Wilcoxon signed-rank test showing that target rule is different from all other rules *among participants who didn't get the correct rule*
eval_summary_other_rules_subset = subset_participants %>%
filter(is_target_rule == FALSE) %>% # for target rule, summarize across participants
group_by(is_target_rule, Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating))
eval_difference_subset = subset_participants %>%
group_by(subjID, Condition) %>%
filter(is_target_rule == TRUE) %>%
inner_join(., eval_summary_other_rules_subset, by = "subjID") %>%
mutate(diff = input_rule_rating - mean_subj_rating) %>%
select(subjID, Condition.x, diff)
wil_exp_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Explain"], exact = F)
wil_des_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Describe"], exact = F)
report_wilcox_summary(wil_exp_subset) # Explainers
report_wilcox_summary(wil_des_subset) # Describers
# 4.1 Evaluation of target rule across conditions (this matches 1 above)
subset_participants = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
filter(Revision == 0) %>%
# filter(Revision == 1) %>%
inner_join(., evaluation_data, on = c("subjID", "Condition"))
# Sanity check the join above
table(subset_participants$subjID) # 6 (number of eval rows) for each subj ID
unique(subset_participants$subjID) # 56 for incorrect rule gen, 30 for correct
sum(generation_free_resp_coded$Revision) # 30 for incorrect rule gen: this is equal to 86 (total) - number of unique participants above or equal to number of unique part. above
# Plot data
# NB: this plot not included in cog sci submission
eval_summary_subset = get_evaluation_summary(subset_participants)
plot_evaluation_results(eval_summary_subset, RULE_EVAL_LABELS)
# Analysis: compare ratings of target rule
t_subset_target = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$is_target_rule == TRUE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_subset_target)
# 4.2. Wilcoxon signed-rank test showing that target rule is different from all other rules *among participants who didn't get the correct rule*
eval_summary_other_rules_subset = subset_participants %>%
filter(is_target_rule == FALSE) %>% # for target rule, summarize across participants
group_by(is_target_rule, Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating))
eval_difference_subset = subset_participants %>%
group_by(subjID, Condition) %>%
filter(is_target_rule == TRUE) %>%
inner_join(., eval_summary_other_rules_subset, by = "subjID") %>%
mutate(diff = input_rule_rating - mean_subj_rating) %>%
select(subjID, Condition.x, diff)
wil_exp_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Explain"], exact = F)
wil_des_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Describe"], exact = F)
report_wilcox_summary(wil_exp_subset) # Explainers
report_wilcox_summary(wil_des_subset) # Describers
# 4.3 Evaluation of distractor rule across conditions (this matches 3.1 above)
t_subset_dist = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == DISTRACTOR_RULE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_subset_dist)
# 4.3 Comparison of distractor rule and target rule across conditions (this matches 3.2 above)
t_subset_target_dist_exp = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
t_subset_target_dist_desc = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_subset_target_dist_exp) # Explainers
report_t_summary(t_subset_target_dist_desc) # Describers
# Test whether this difference is significant with ANOVA
subset_participants_distractor_target = subset_participants %>%
filter(is_target_rule == T | rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish.")
# sanity check
unique(subset_participants_distractor_target$rule_text)
table(subset_participants_distractor_target$subjID)
# check for significant interaction between condition and target rule v. distractor
interaction_test = aov(data = subset_participants_distractor_target, input_rule_rating ~ Condition*is_target_rule)
summary(interaction_test)
# getting DF for reporting F statistics in Anova above
unique(subset_participants_distractor_target$subjID)
# 1. Memory performance compared to chance
# NB: not doing binomial test here because we are looking at accuracy percentages for N subjects
t_mem_chance_exp = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
mu = 0.5,
equal.var = T)
t_mem_chance_desc = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
mu = 0.5,
equal.var = T)
report_t_summary(t_mem_chance_exp)
report_t_summary(t_mem_chance_desc)
# 2. Memory accuracy across conditions
# NB: this plot not included in cog sci submission
# TODO split out this chart and analysis by positive and negative probes?
plot_memory_data(memory_summary)
t_mem = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
var.equal = T)
report_t_summary(t_mem)
# 3. Memory performance comparing participants who did and didn't get the correct rule
memory_hypothesis_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., memory_subject_summary, on = subjID)
t_mem_correct = t.test(memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 0],
memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 1],
var.equal = T)
report_t_summary(t_mem_correct)
# 1. Overall time on task across conditions
# NB: this generates the plot but we display below with patchwork
time_on_task = plot_time_data(completion_time_summary, ylab = "Seconds", ymax = 1000, title = "Mean time on experiment")
t_time = t.test(summary_data$experiment_completion_time[summary_data$Condition == "Describe"],
summary_data$experiment_completion_time[summary_data$Condition == "Explain"],
var.equal = T)
report_t_summary(t_time) # Means are seconds on task
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
unique(trial_time_subject_summary$subjID[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Round1[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Condition[is.na(trial_time_subject_summary$mean_trial_completion)])
# NB: this generates the plot but we display below with patchwork
time_on_trials = plot_time_data(trial_time_summary, ylab = "Seconds", ymax = 80, title = "Mean time on trials")
t_trials = t.test(trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Describe" &
trial_time_subject_summary$Round1 == FALSE],
trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Explain" &
trial_time_subject_summary$Round1 == FALSE],
var.equal = T)
report_t_summary(t_trials) # Means are seconds on trials
# Plot graphs from 1. and 2. above side by side with patchwork
time_on_task + time_on_trials
# 3. Overall time on task comparing participants who did and didn't get the correct rule
task_time_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., summary_data, on = c("subjID", "Condition")) %>%
select(subjID, Condition, Revision, experiment_completion_time)
t_task_time_correct = t.test(task_time_join$experiment_completion_time[task_time_join$Revision == 0],
task_time_join$experiment_completion_time[task_time_join$Revision == 1],
var.equal = T)
report_t_summary(t_task_time_correct) # Means are seconds on task
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
unique(trial_time_subject_summary$subjID[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Round1[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Condition[is.na(trial_time_subject_summary$mean_trial_completion)])
trial_time_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., trial_time_subject_summary, on = c("subjID", "Condition")) %>%
select(subjID, Condition, Revision, mean_trial_completion)
t_trial_time_correct = t.test(trial_time_join$mean_trial_completion[trial_time_join$Revision == 0],
trial_time_join$mean_trial_completion[trial_time_join$Revision == 1],
var.equal = T)
report_t_summary(t_trial_time_correct) # Means are seconds on trials
plot_prediction_summary(prediction_summary)
# Fit regressions to data just to ensure no signal
# TODO if including this, revise models to only look at trials > 4
# (or change graph above to look at all trials)
mod_exp = lm(data = prediction_summary[prediction_summary$Condition == "Explain",], accuracy ~ trial_index)
mod_des = lm(data = prediction_summary[prediction_summary$Condition == "Describe",], accuracy ~ trial_index)
summary(mod_exp)
summary(mod_des)
# See how our coding compared to accuracy in the classification task
generation_task_comparison = generation_free_resp_coded %>%
inner_join(generation_judgment_subject_summary, by = c("subjID", "Condition"))
# How many people did we code as getting the rule who were < 100% on the classification?
generation_task_comparison %>%
filter(Revision == 1 & subj_accuracy < 1)
# How many people got 100% on the classification but were coded as not getting the rule?
generation_task_comparison %>%
filter(Revision == 0 & subj_accuracy == 1)
# Plot relationship between coded accuracy and classification accuracy
generation_task_comparison %>%
ggplot(aes(x = subj_accuracy, y = Revision, color = Condition)) +
geom_jitter(width = 0.05, height = 0.05) +
scale_color_viridis(discrete = T) +
individ_plot_theme
# Correlation between the two
cor.test(generation_task_comparison$Revision, generation_task_comparison$subj_accuracy)
plot_evaluation_histogram = function(eval_data) {
eval_data %>%
ggplot(aes(x = input_rule_rating)) +
geom_histogram(breaks = c(1, 2, 3, 4, 5, 6, 7),
position = "identity",
color = "lightblue", fill = "lightblue", alpha = 0.75) +
labs(x = "Evaluation rating (1-7)") +
individ_plot_theme
}
# ALL participants
# Explain condition, target rule
evaluation_data %>%
filter(Condition == "Explain" & is_target_rule == T) %>%
plot_evaluation_histogram()
# Describe condition, target rule
evaluation_data %>%
filter(Condition == "Describe" & is_target_rule == T) %>%
plot_evaluation_histogram()
# ONLY participants who did not generate correct target rule
# Explain condition, target rule
subset_participants %>%
filter(Condition == "Explain" & is_target_rule == T) %>%
plot_evaluation_histogram()
# Describe condition, target rule
subset_participants %>%
filter(Condition == "Describe" & is_target_rule == T) %>%
plot_evaluation_histogram()
# Plot eval ratings of target rule among participants who did and didn't get correct rule in each condition
breakout_participants = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., evaluation_data, on = subjID)
subset_summary = breakout_participants %>%
filter(is_target_rule == T) %>%
# filter(rule_text == DISTRACTOR_RULE) %>%
group_by(Condition, as.factor(Revision)) %>%
summarize(mean_rating = mean(input_rule_rating),
n = n(),
se = sd(input_rule_rating) / sqrt(n),
ci_lower = mean_rating - se,
ci_upper = mean_rating + se) %>%
rename(Revision = `as.factor(Revision)`)
breakout_participants %>%
filter(is_target_rule == T) %>%
# filter(rule_text == DISTRACTOR_RULE) %>%
ggplot(aes(x = Condition, y = input_rule_rating, color = as.factor(Revision))) +
geom_jitter(width = 0.25, alpha = 0.5, height = 0.25, size = 4) +
geom_point(data = subset_summary,
aes(x = Condition, y = mean_rating, color = Revision),
size = 6) +
geom_errorbar(data = subset_summary,
aes(x = Condition, y = mean_rating, color = Revision, ymin = ci_lower, ymax = ci_upper),
width = 0.25,
size = 1) +
scale_color_viridis(discrete = T,
labels = c("0" = "Incorrect hypothesis", "1" = "Correct hypothesis"),
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_y_discrete(breaks = c(2, 3, 4, 5, 6, 7),
labels = c("2" = "2", "3" = "3", "4" = "4", "5" = "5", "6" = "6", "7" = "7"),
limits = c(2, 3, 4, 5, 6, 7)) +
labs(y = "Rating (1-7)") +
ggtitle("Target rule evaluation") +
individ_plot_theme
# 3.1 Evaluation of distractor rule across conditions
t_eval_comp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == DISTRACTOR_RULE],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_eval_comp)
# 3.2 Comparison of distractor rule and target rule across conditions
t_eval_target_dist_exp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
t_eval_target_dist_desc = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_eval_target_dist_exp) # Explainers
report_t_summary(t_eval_target_dist_desc) # Describers
# Test whether this difference is significant with ANOVA
eval_data_distractor_target = evaluation_data %>%
filter(is_target_rule == T | rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish.")
# sanity check
unique(eval_data_distractor_target$rule_text)
table(eval_data_distractor_target$subjID)
interaction_test = aov(data = eval_data_distractor_target, input_rule_rating ~ Condition*is_target_rule)
summary(interaction_test)
# getting DF for reporting F statistics in Anova above
unique(eval_data_distractor_target$subjID)
summary(interaction_test)
