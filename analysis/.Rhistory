scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_y_continuous(breaks = seq(1, 7)) +
individ_plot_theme
}
# Bar chart of experiment completion time or avg. trial time
plot_time_data = function(time_summary, ylab, ymax, title) {
time_summary %>%
ggplot(aes(x = Condition, y = mean_task_time,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
ylim(0, ymax) +
labs(x = "", y = ylab) +
ggtitle(title) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank(),
plot.title = element_text(size = 32, face = "bold"))
}
# Bar chart of average memory accuracy across conditions
plot_memory_data = function(memory_summary) {
memory_summary %>%
ggplot(aes(x = Condition, y = mean_memory_accuracy,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
geom_hline(yintercept = 0.5, linetype = "dashed", size = 1) +
ylim(c(0, 1)) +
labs(x = "", y = "Mean memory accuracy") +
# ggtitle("Memory probe accuracy across conditions") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Bar chart of counts of feature references in coded explanations/descriptions, by condition
plot_coded_explanation_data = function(explanation_coded_summary) {
explanation_coded_summary %>%
ggplot(aes(x = measure, y = measure_mean,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", position = position_dodge(preserve = "single"),
width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = measure_mean - measure_se, ymax = measure_mean + measure_se),
position = position_dodge(width = 0.5, preserve = "single"),
width = 0.25) +
ggtitle("Explanation and description measures") +
labs(x = "", y = "Avg. count") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme
}
# Read in data
summary_data = bind_rows(read_summary_data(ROUND1_SUMMARY_DATA, TRUE),
read_summary_data(ROUND2_SUMMARY_DATA, FALSE))
trial_data = bind_rows(read_trial_data(ROUND1_TRIAL_DATA, TRUE),
read_trial_data(ROUND2_TRIAL_DATA, FALSE))
generation_free_resp_coded = read_coded_free_resp_data(GENERATION_RESP_DATA_CODED, summary_data)
generation_judgment_data = bind_rows(read_generation_judgment_data(ROUND1_GENERATION_JUDG_DATA, TRUE),
read_generation_judgment_data(ROUND2_GENERATION_JUDG_DATA, FALSE))
evaluation_data = bind_rows(read_evaluation_data(ROUND1_EVAL_DATA, TRUE),
read_evaluation_data(ROUND2_EVAL_DATA, FALSE))
memory_data = bind_rows(read_memory_data(ROUND1_MEMORY_DATA, TRUE),
read_memory_data(ROUND2_MEMORY_DATA, FALSE))
# Summarize data
prediction_summary = get_prediction_summary(trial_data)
generation_free_resp_summary = get_generation_free_response_summary(generation_free_resp_coded)
generation_judgment_subject_summary = get_generation_judgment_subj_summary(generation_judgment_data)
generation_judgment_summary = get_generation_judgment_summary(generation_judgment_subject_summary)
evaluation_summary = get_evaluation_summary(evaluation_data)
prediction_summary
generation_free_resp_summary
generation_judgment_subject_summary
generation_judgment_summary
evaluation_summary
completion_time_summary = get_time_summary(summary_data)
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
trial_time_subject_summary = get_trial_time_subj_summary(trial_data)
trial_time_summary = get_trial_time_summary(trial_time_subject_summary)
memory_subject_summary = get_memory_subj_summary(memory_data)
memory_summary = get_memory_summary(memory_subject_summary)
# Coded explanation / description data
explanation_coded_data = read_coded_explanation_data(EXPLANATION_DATA_CODED)
explanation_coded_summary_subjects = get_explanation_coded_subj_summary(explanation_coded_data)
explanation_coded_summary = get_explanation_coded_summary(explanation_coded_summary_subjects)
# 1. Generation free response task
# NB: this plot not included in cog sci submission, but statistics are reported
generation_fr = plot_generation_free_responses(generation_free_resp_summary)
generation_props = generation_free_resp_coded %>%
group_by(Condition) %>%
summarize(success = sum(Revision),
total = n())
chisq_gen = prop.test(c(generation_props$success), c(generation_props$total))
report_chisq_summary(chisq_gen)
# 2. Generation judgment task: overall accuracy across conditions
generation_class = plot_generation_judgments(generation_judgment_summary)
t_gen = t.test(
generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Describe"],
generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Explain"],
var.equal = T
)
report_t_summary(t_gen)
# 3. Generation judgment task: proportion of people who got 100% across conditions
# NB: similar Chi-sq test with raw count of people who were *coded as getting rule* is what we report above
# for the free response task so this is effectively a parallel analysis for people who got 100% on judgment task
generation_task_comparison = generation_free_resp_coded %>%
inner_join(generation_judgment_subject_summary, by = c("subjID", "Condition"))
# NB: we want table to be subj_accuracy == 1 but we do < 1 below to get count of == 1 as the *first* column
count_data = table(generation_task_comparison$Condition, generation_task_comparison$subj_accuracy < 1)
chisq.test(count_data) # NB: this is equivalent to prop.test(count_data)
generation_fr + generation_class
# 1. Evaluation of target rule across conditions
plot_evaluation_results(evaluation_summary, RULE_EVAL_LABELS)
t_eval = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe" & evaluation_data$is_target_rule == TRUE],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain" & evaluation_data$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_eval)
# 2. Wilcoxon signed-rank test showing that target rule is different from all other rules across both groups
eval_summary_other_rules = evaluation_data %>%
filter(is_target_rule == FALSE) %>%
group_by(is_target_rule, Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating))
eval_difference = evaluation_data %>%
group_by(subjID, Condition) %>%
filter(is_target_rule == TRUE) %>%
inner_join(., eval_summary_other_rules, by = "subjID") %>%
mutate(diff = input_rule_rating - mean_subj_rating) %>%
select(subjID, Condition.x, diff)
wil_exp = wilcox.test(eval_difference$diff[eval_difference$Condition.x == "Explain"], exact = F)
wil_des = wilcox.test(eval_difference$diff[eval_difference$Condition.x == "Describe"], exact = F)
report_wilcox_summary(wil_exp) # Explainers
report_wilcox_summary(wil_des) # Describers
# 3.1 Evaluation of distractor rule across conditions
t_eval_comp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == DISTRACTOR_RULE],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_eval_comp)
# 3.2 Comparison of distractor rule and target rule across conditions
t_eval_target_dist_exp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
t_eval_target_dist_desc = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_eval_target_dist_exp) # Explainers
report_t_summary(t_eval_target_dist_desc) # Describers
# Test whether this difference is significant with ANOVA
eval_data_distractor_target = evaluation_data %>%
filter(is_target_rule == T | rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish.")
# sanity check
unique(eval_data_distractor_target$rule_text)
table(eval_data_distractor_target$subjID)
# check for significant interaction between condition and target rule v. distractor
interaction_test = aov(data = eval_data_distractor_target, input_rule_rating ~ Condition*is_target_rule)
summary(interaction_test)
# getting DF for reporting F statistics in Anova above
unique(eval_data_distractor_target$subjID)
# 4.1 Evaluation of target rule across conditions (this matches 1 above)
subset_participants = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
filter(Revision == 0) %>%
# filter(Revision == 1) %>%
inner_join(., evaluation_data, by = c("subjID", "Condition"))
# Sanity check the join above
table(subset_participants$subjID) # 6 (number of eval rows) for each subj ID
unique(subset_participants$subjID) # 56 for incorrect rule gen, 30 for correct
sum(generation_free_resp_coded$Revision) # 30 for incorrect rule gen: this is equal to 86 (total) - number of unique participants above or equal to number of unique part. above
# Plot data
# NB: this plot not included in cog sci submission
eval_summary_subset = get_evaluation_summary(subset_participants)
plot_evaluation_results(eval_summary_subset, RULE_EVAL_LABELS)
# Analysis: compare ratings of target rule
t_subset_target = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$is_target_rule == TRUE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_subset_target)
# 4.2. Wilcoxon signed-rank test showing that target rule is different from all other rules *among participants who didn't get the correct rule*
eval_summary_other_rules_subset = subset_participants %>%
filter(is_target_rule == FALSE) %>% # for target rule, summarize across participants
group_by(is_target_rule, Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating))
eval_difference_subset = subset_participants %>%
group_by(subjID, Condition) %>%
filter(is_target_rule == TRUE) %>%
inner_join(., eval_summary_other_rules_subset, by = "subjID") %>%
mutate(diff = input_rule_rating - mean_subj_rating) %>%
select(subjID, Condition.x, diff)
wil_exp_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Explain"], exact = F)
wil_des_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Describe"], exact = F)
report_wilcox_summary(wil_exp_subset) # Explainers
report_wilcox_summary(wil_des_subset) # Describers
# 4.3 Evaluation of distractor rule across conditions (this matches 3.1 above)
t_subset_dist = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == DISTRACTOR_RULE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_subset_dist)
# 4.3 Comparison of distractor rule and target rule across conditions (this matches 3.2 above)
t_subset_target_dist_exp = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
t_subset_target_dist_desc = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_subset_target_dist_exp) # Explainers
report_t_summary(t_subset_target_dist_desc) # Describers
# Test whether this difference is significant with ANOVA
subset_participants_distractor_target = subset_participants %>%
filter(is_target_rule == T | rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish.")
# sanity check
unique(subset_participants_distractor_target$rule_text)
table(subset_participants_distractor_target$subjID)
# check for significant interaction between condition and target rule v. distractor
interaction_test = aov(data = subset_participants_distractor_target, input_rule_rating ~ Condition*is_target_rule)
summary(interaction_test)
# getting DF for reporting F statistics in Anova above
unique(subset_participants_distractor_target$subjID)
# 1. Memory performance compared to chance
# NB: not doing binomial test here because we are looking at accuracy percentages for N subjects
t_mem_chance_exp = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
mu = 0.5,
equal.var = T)
t_mem_chance_desc = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
mu = 0.5,
equal.var = T)
report_t_summary(t_mem_chance_exp)
report_t_summary(t_mem_chance_desc)
# 2. Memory accuracy across conditions
# NB: this plot not included in cog sci submission
# TODO split out this chart and analysis by positive and negative probes?
plot_memory_data(memory_summary)
# 2. Memory accuracy across conditions
# NB: this plot not included in cog sci submission
# TODO split out this chart and analysis by positive and negative probes?
mem = plot_memory_data(memory_summary)
# 2. Memory accuracy across conditions
# NB: this plot not included in cog sci submission
# TODO split out this chart and analysis by positive and negative probes?
mem = plot_memory_data(memory_summary)
t_mem = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
var.equal = T)
report_t_summary(t_mem)
# Bar chart of average memory accuracy across conditions
plot_memory_data = function(memory_summary) {
memory_summary %>%
ggplot(aes(x = Condition, y = mean_memory_accuracy,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
geom_hline(yintercept = 0.5, linetype = "dashed", size = 1) +
ylim(c(0, 1)) +
labs(x = "", y = "Accuracy") +
ggtitle("Mean memory performance") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# 2. Memory accuracy across conditions
# NB: this plot not included in cog sci submission
# TODO split out this chart and analysis by positive and negative probes?
mem = plot_memory_data(memory_summary)
mem
# 2. Memory accuracy across conditions
# NB: this plot not included in cog sci submission
# TODO split out this chart and analysis by positive and negative probes?
mem = plot_memory_data(memory_summary)
t_mem = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
var.equal = T)
report_t_summary(t_mem)
# 3. Memory performance comparing participants who did and didn't get the correct rule
memory_hypothesis_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., memory_subject_summary, by = c("subjID"))
t_mem_correct = t.test(memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 0],
memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 1],
var.equal = T)
report_t_summary(t_mem_correct)
# 1. Overall time on task across conditions
# NB: this generates the plot but we display below with patchwork
time_on_task = plot_time_data(completion_time_summary, ylab = "Seconds", ymax = 1000, title = "Mean time on experiment")
t_time = t.test(summary_data$experiment_completion_time[summary_data$Condition == "Describe"],
summary_data$experiment_completion_time[summary_data$Condition == "Explain"],
var.equal = T)
report_t_summary(t_time) # Means are seconds on task
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
unique(trial_time_subject_summary$subjID[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Round1[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Condition[is.na(trial_time_subject_summary$mean_trial_completion)])
# NB: this generates the plot but we display below with patchwork
time_on_trials = plot_time_data(trial_time_summary, ylab = "Seconds", ymax = 80, title = "Mean time on trials")
t_trials = t.test(trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Describe" &
trial_time_subject_summary$Round1 == FALSE],
trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Explain" &
trial_time_subject_summary$Round1 == FALSE],
var.equal = T)
report_t_summary(t_trials) # Means are seconds on trials
# Plot graphs from 1. and 2. above side by side with patchwork
time_on_task + time_on_trials
# Plot graphs from 1. and 2. above side by side with patchwork
time_on_task + time_on_trials + mem
# Bar chart of experiment completion time or avg. trial time
plot_time_data = function(time_summary, ylab, ymax, title) {
time_summary %>%
ggplot(aes(x = Condition, y = mean_task_time,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
ylim(0, ymax) +
labs(x = "", y = ylab) +
ggtitle(title) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
#plot.title = element_text(size = 32, face = "bold"))
}
# 2. Memory accuracy across conditions
# NB: this plot not included in cog sci submission
# TODO split out this chart and analysis by positive and negative probes?
mem = plot_memory_data(memory_summary)
t_mem = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
var.equal = T)
report_t_summary(t_mem)
# 3. Memory performance comparing participants who did and didn't get the correct rule
memory_hypothesis_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., memory_subject_summary, by = c("subjID"))
t_mem_correct = t.test(memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 0],
memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 1],
var.equal = T)
report_t_summary(t_mem_correct)
# 1. Overall time on task across conditions
# NB: this generates the plot but we display below with patchwork
time_on_task = plot_time_data(completion_time_summary, ylab = "Seconds", ymax = 1000, title = "Mean time on experiment")
t_time = t.test(summary_data$experiment_completion_time[summary_data$Condition == "Describe"],
summary_data$experiment_completion_time[summary_data$Condition == "Explain"],
var.equal = T)
report_t_summary(t_time) # Means are seconds on task
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
unique(trial_time_subject_summary$subjID[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Round1[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Condition[is.na(trial_time_subject_summary$mean_trial_completion)])
# NB: this generates the plot but we display below with patchwork
time_on_trials = plot_time_data(trial_time_summary, ylab = "Seconds", ymax = 80, title = "Mean time on trials")
t_trials = t.test(trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Describe" &
trial_time_subject_summary$Round1 == FALSE],
trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Explain" &
trial_time_subject_summary$Round1 == FALSE],
var.equal = T)
report_t_summary(t_trials) # Means are seconds on trials
# Plot graphs from 1. and 2. above side by side with patchwork
time_on_task + time_on_trials + mem
# Plot graphs from 1. and 2. above side by side with patchwork
mem + time_on_task + time_on_trials
# 3. Overall time on task comparing participants who did and didn't get the correct rule
task_time_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., summary_data, by = c("subjID", "Condition")) %>%
select(subjID, Condition, Revision, experiment_completion_time)
t_task_time_correct = t.test(task_time_join$experiment_completion_time[task_time_join$Revision == 0],
task_time_join$experiment_completion_time[task_time_join$Revision == 1],
var.equal = T)
report_t_summary(t_task_time_correct) # Means are seconds on task
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
unique(trial_time_subject_summary$subjID[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Round1[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Condition[is.na(trial_time_subject_summary$mean_trial_completion)])
trial_time_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., trial_time_subject_summary, by = c("subjID", "Condition")) %>%
select(subjID, Condition, Revision, mean_trial_completion)
t_trial_time_correct = t.test(trial_time_join$mean_trial_completion[trial_time_join$Revision == 0],
trial_time_join$mean_trial_completion[trial_time_join$Revision == 1],
var.equal = T)
report_t_summary(t_trial_time_correct) # Means are seconds on trials
# sanity checks
glimpse(explanation_coded_data)
glimpse(explanation_coded_summary_subjects)
glimpse(explanation_coded_summary)
unique(explanation_coded_data$Subject)
# Plot results
plot_coded_explanation_data(explanation_coded_summary)
# Bar chart of counts of feature references in coded explanations/descriptions, by condition
plot_coded_explanation_data = function(explanation_coded_summary) {
explanation_coded_summary %>%
ggplot(aes(x = measure, y = measure_mean,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", position = position_dodge(preserve = "single"),
width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = measure_mean - measure_se, ymax = measure_mean + measure_se),
position = position_dodge(width = 0.5, preserve = "single"),
width = 0.25) +
ggtitle("Explanation and description measures") +
labs(x = "", y = "Mean number of references") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme
}
# Plot results
plot_coded_explanation_data(explanation_coded_summary)
# Plot graphs from 1. and 2. above side by side with patchwork
# time_on_task + time_on_trials
mem + time_on_task + time_on_trials + plot_annotation(tag_levels = 'A')
# Plot graphs from 1. and 2. above side by side with patchwork
# time_on_task + time_on_trials
mem + time_on_task + time_on_trials +
plot_annotation(tag_levels = 'A') +
theme(plot.tag = element_text(size = 8))
# Plot graphs from 1. and 2. above side by side with patchwork
# time_on_task + time_on_trials
mem + time_on_task + time_on_trials +
plot_annotation(tag_levels = 'A') +
theme(plot.tag = element_text(size = 20))
# Plot graphs from 1. and 2. above side by side with patchwork
# time_on_task + time_on_trials
mem + time_on_task + time_on_trials +
plot_annotation(tag_levels = 'A') &
theme(plot.tag = element_text(size = 20))
# Plot graphs from 1. and 2. above side by side with patchwork
# time_on_task + time_on_trials
mem + time_on_task + time_on_trials +
plot_annotation(tag_levels = 'A') &
theme(plot.tag = element_text(size = 20, face = "bold"))
1965 + 277 + 271 + 323 + 609 + 407 + 427 + 887 + 420 + 1100
interaction_test
summary(interaction_test)
2016 + 277 + 271 + 323 + 613 + 407 + 427 + 892 + 420 + 1106
