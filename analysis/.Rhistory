ROUND1_EVAL_DATA = "round1/05_go_fish_evaluation.csv"
ROUND1_MEMORY_DATA = "round1/06_go_fish_memory.csv"
ROUND2_SUMMARY_DATA = "round2/01_go_fish_meta.csv"
ROUND2_TRIAL_DATA = "round2/02_go_fish_trials.csv"
ROUND2_GENERATION_RESP_DATA = "round2/03_go_fish_generation_free_resp.csv"
ROUND2_GENERATION_JUDG_DATA = "round2/04_go_fish_generation_judgment.csv"
ROUND2_EVAL_DATA = "round2/05_go_fish_evaluation.csv"
ROUND2_MEMORY_DATA = "round2/06_go_fish_memory.csv"
GENERATION_RESP_DATA_CODED = "free_response_combined.csv"
EXPLANATION_DATA_CODED = "explanation_coded.csv"
# Data labels
RULE_EVAL_LABELS = c("TRUE" = "Target rule", "FALSE" = "All other rules")
DISTRACTOR_LABELS = c("TRUE" = "Target rule", "FALSE" = "Distractor rule")
DISTRACTOR_RULE = "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."
# the memory probe item with this shape config was erroneously coded as being in experiment when it wasn't
# we correct the `memory_shape_in_expt` column value when reading in the data
MEMORY_MISCUE = "{'top_shape': 'diamond', 'top_color': 'blue', 'top_texture': False, 'bottom_shape': 'circle', 'bottom_color': 'blue', 'bottom_texture': False}"
CODED_MEASURE_FILTER = c("shape_total", "color_total", "purple_dot_total")
coded_measure_width = 10
CODED_MEASURE_LOOKUP = c("mechanism_total" = str_wrap("Mechanism", width = coded_measure_width),
# "shape_total" = str_wrap("Shape (total)", width = coded_measure_width),
"shape_abstract_total" = str_wrap("Shape (abstract)", width = coded_measure_width),
"shape_concrete_total" = str_wrap("Shape (concrete)", width = coded_measure_width),
# "color_total" = str_wrap("Color (total)", width = coded_measure_width),
"color_abstract_total" = str_wrap("Color (abstract)", width = coded_measure_width),
"color_concrete_total" = str_wrap("Color (concrete)", width = coded_measure_width),
# "purple_dot_total" = str_wrap("Purple dot (total)", width = coded_measure_width),
"purple_dot_abstract_total" = str_wrap("Purple dot (abstract)", width = coded_measure_width),
"purple_dot_concrete_total" = str_wrap("Purple dot (concrete)", width = coded_measure_width)
)
CODED_MEASURE_LEVELS = c(str_wrap("Mechanism", width = coded_measure_width),
# str_wrap("Shape (total)", width = coded_measure_width),
str_wrap("Shape (abstract)", width = coded_measure_width), str_wrap("Shape (concrete)", width = coded_measure_width),
# str_wrap("Color (total)", width = coded_measure_width),
str_wrap("Color (abstract)", width = coded_measure_width), str_wrap("Color (concrete)", width = coded_measure_width),
# str_wrap("Purple dot (total)", width = coded_measure_width),
str_wrap("Purple dot (abstract)", width = coded_measure_width), str_wrap("Purple dot (concrete)", width = coded_measure_width))
# Read in and process summary data file
read_summary_data = function(filepath, is_round1) {
# Read in data at filepath and add/modify columns as needed
summary_data = read_csv(filepath)
summary_data = summary_data %>%
mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
Round1 = is_round1,
experiment_completion_time = (expt_end_ts - expt_start_ts) / 1000)
return(summary_data)
}
# Read in and process data from explain/describe trials
read_trial_data = function(filepath, is_round1) {
trial_data = read_csv(filepath)
trial_data = trial_data %>%
mutate(
Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
Round1 = is_round1,
input_correct =
(input_prediction_catches_fish == prediction_catches_fish))
trial_data$input_evidence_response = str_replace_all(
trial_data$input_evidence_response, "\n" , "[newline]")
return(trial_data)
}
# Read in and process data from generation "judgment" (binary response) task
read_generation_judgment_data = function(filepath, is_round1) {
generation_judgment_data = read_csv(filepath)
generation_judgment_data = generation_judgment_data %>%
mutate(
Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
Round1 = is_round1,
input_correct =
(input_judgment == judgment_catches_fish))
return(generation_judgment_data)
}
# Read in and process data from rule evaluation task
read_evaluation_data = function(filepath, is_round1) {
evaluation_data = read_csv(filepath)
evaluation_data = evaluation_data %>%
mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
Round1 = is_round1)
return(evaluation_data)
}
# Read in and process data from memory task at end of experiment
read_memory_data = function(filepath, is_round1) {
memory_data = read_csv(filepath)
memory_data = memory_data %>%
mutate(Condition = ifelse(is_control == TRUE, "Describe", "Explain"),
Round1 = is_round1,
memory_correct =
(memory_shape_in_expt == input_shape_in_expt))
# Fix mis-coded memory probe
memory_data %>%
filter(memory_shape == MEMORY_MISCUE) %>%
mutate(memory_shape_in_expt = 0)
return(memory_data)
}
# Read in coded free response data (note this requires writing free response data to csv in initialization code)
read_coded_free_resp_data = function(filename, summary_data) {
generation_free_resp_coded = read_csv(filename)
generation_free_resp_coded = generation_free_resp_coded %>%
select(subjID, free_response_str, Revision) %>%
inner_join(., summary_data, by = "subjID") %>%
select(subjID, Condition, free_response_str, Revision)
return(generation_free_resp_coded)
}
# Read in coded explanation/description data
read_coded_explanation_data = function(filename) {
explanation_free_resp_coded = read_csv(filename)
return(explanation_free_resp_coded)
}
# Summarize prediction data for graphing
get_prediction_summary = function(trial_data) {
prediction_summary = trial_data %>%
group_by(Condition, trial_index) %>%
summarize(accuracy = sum(input_correct) / n())
return(prediction_summary)
}
# Summarize generation judgment data by participant
get_generation_judgment_subj_summary = function(generation_judgment_data) {
generation_subject_summary = generation_judgment_data %>%
group_by(Condition, subjID) %>%
summarize(subj_accuracy = sum(input_correct) / n())
return(generation_subject_summary)
}
# Summarize generation judgment data across participants
get_generation_judgment_summary = function(generation_judgment_subject_summary) {
generation_judg_summary = generation_judgment_subject_summary %>%
group_by(Condition) %>%
summarize(mean_accuracy = mean(subj_accuracy),
subjects = n(),
se_accuracy = sd(subj_accuracy) / sqrt(n()),
ci_lower = mean_accuracy - se_accuracy,
ci_upper = mean_accuracy + se_accuracy)
return(generation_judg_summary)
}
# Summarize generation free response data across participants
get_generation_free_response_summary = function(generation_free_response_coded) {
generation_free_response_summary = generation_free_response_coded %>%
group_by(Condition) %>%
summarize(subjects = n(),
correct_generation_pct = sum(Revision) / n())
return(generation_free_response_summary)
}
# Summarize evaluation data across participants and conditions for target and non-target rules
get_evaluation_summary = function(evaluation_data) {
# Average rating across participants on target rule
eval_summary_target_rule = evaluation_data %>%
filter(is_target_rule == TRUE) %>% # for target rule, summarize across participants
group_by(Condition) %>%
summarize(ruleset = "Target",
mean_rating = mean(input_rule_rating),
subjects = n(),
se_rating = sd(input_rule_rating) / sqrt(n()),
ci_lower = mean_rating - se_rating,
ci_upper = mean_rating + se_rating)
# Average rating across participants on distractor rule
eval_summary_distractor_rule = evaluation_data %>%
filter(rule_text == DISTRACTOR_RULE) %>% # for distractor rule, summarize across participants
group_by(Condition) %>%
summarize(ruleset = "Distractor",
mean_rating = mean(input_rule_rating),
subjects = n(),
se_rating = sd(input_rule_rating) / sqrt(n()),
ci_lower = mean_rating - se_rating,
ci_upper = mean_rating + se_rating)
# Average of each participant's average across non-target rules
eval_summary_other_rules = evaluation_data %>%
filter(is_target_rule == FALSE &
rule_text != DISTRACTOR_RULE) %>% # for all other rules, summarize average of participant avgs
group_by(Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating),
rules = n()) %>%
group_by(Condition) %>%
summarize(ruleset = "All other rules",
mean_rating = mean(mean_subj_rating),
subjects = n(),
se_rating = sd(mean_subj_rating) / sqrt(n()),
ci_lower = mean_rating - se_rating,
ci_upper = mean_rating + se_rating)
eval_summary = rbind(eval_summary_target_rule, eval_summary_distractor_rule, eval_summary_other_rules)
return(eval_summary)
}
# Summarize experiment completion time data
get_time_summary = function(time_data) {
time_summary = time_data %>%
group_by(Condition) %>%
summarize(mean_task_time = mean(experiment_completion_time),
subjects = n(),
se_task_time = sd(experiment_completion_time) / sqrt(subjects),
ci_lower = mean_task_time - se_task_time,
ci_upper = mean_task_time + se_task_time)
return(time_summary)
}
# Summarize average trial completion time by participant
get_trial_time_subj_summary = function(trial_data) {
trial_subject_summary = trial_data %>%
mutate(trial_completion_time = (trial_n_end_ts - trial_n_start_ts) / 1000) %>%
group_by(Round1, Condition, subjID) %>%
summarize(mean_trial_completion = mean(trial_completion_time),
trials = n(),
se_trial_completion = sd(trial_completion_time) / sqrt(trials),
ci_lower = mean_trial_completion - se_trial_completion,
ci_upper = mean_trial_completion + se_trial_completion)
return(trial_subject_summary)
}
# Get summary of time spent on trials in each condition across participants
get_trial_time_summary = function(trial_time_subj_summary) {
trial_time_summary = trial_time_subj_summary %>%
# trial time completion not available in round1 data
filter(Round1 == FALSE) %>%
group_by(Condition) %>%
# These column names kept the same as those in `get_time_summary` for easier graphing
summarize(mean_task_time = mean(mean_trial_completion),
subjects = n(),
se_trial_time = sd(mean_trial_completion) / sqrt(subjects),
ci_lower = mean_task_time - se_trial_time,
ci_upper = mean_task_time + se_trial_time)
return(trial_time_summary)
}
# Summarize memory performance data by participant
get_memory_subj_summary = function(memory_data) {
memory_subj_accuracy = memory_data %>%
group_by(Condition, subjID) %>%
summarize(subj_accuracy = sum(memory_correct) / n())
return(memory_subj_accuracy)
}
# Summarize memory performance across participants by condition
get_memory_summary = function(memory_subject_summary) {
memory_summary = memory_subject_summary %>%
group_by(Condition) %>%
summarize(mean_memory_accuracy = mean(subj_accuracy),
subjects = n(),
se_memory_accuracy = sd(subj_accuracy) / sqrt(n()),
ci_lower = mean_memory_accuracy - se_memory_accuracy,
ci_upper = mean_memory_accuracy + se_memory_accuracy)
return(memory_summary)
}
# Summarize explanation/description coded data
get_explanation_coded_subj_summary = function(explanation_data) {
explanation_summary = explanation_data %>%
group_by(Condition, Subject) %>%
summarize(mechanism_total = sum(`FINAL - total mechanisms`),
shape_total = sum(`FINAL - total shape references`),
shape_abstract_total = sum(`FINAL - abstract shape references`),
shape_concrete_total = sum(`FINAL - concrete shape references`),
color_total = sum(`FINAL - total color references`),
color_abstract_total = sum(`FINAL - abstract color references`),
color_concrete_total = sum(`FINAL - concrete color references`),
purple_dot_total = sum(`FINAL - total purple dot references`),
purple_dot_abstract_total = sum(`FINAL - abstract purple dot references`),
purple_dot_concrete_total = sum(`FINAL - concrete purple dot references`)) %>%
gather(
key = "measure",
value = "subject_total",
-Condition,
-Subject
)
return(explanation_summary)
}
# Summarize coded explanation/description results across participants by condition
get_explanation_coded_summary = function(explanation_subject_summary) {
explanation_summary = explanation_subject_summary %>%
filter(!measure %in% CODED_MEASURE_FILTER) %>%
mutate(measure = factor(CODED_MEASURE_LOOKUP[measure], levels = CODED_MEASURE_LEVELS)) %>%
group_by(Condition, measure) %>%
summarize(subjects = n(),
measure_mean = mean(subject_total),
measure_se = sd(subject_total) / sqrt(subjects)
)
return(explanation_summary)
}
# Auxiliary function for printing out t test statistics
report_t_summary = function(t_test) {
paste("Mean 1:", round(t_test$estimate[1], 2), "||",
"Mean 2:", round(t_test$estimate[2], 2), "||",
"t (", t_test$parameter, ") =", round(t_test$statistic, 2), ",",
"p =", round(t_test$p.value, 3),
sep = " ")
}
# Auxiliary function for printing out chi sq test statistics
report_chisq_summary = function(chisq_test) {
paste("Count/Prop 1:", round(chisq_test$estimate[1], 3), "||",
"Count/Prop 2:", round(chisq_test$estimate[2], 3), "||",
"X^2 (", chisq_test$parameter, ") =", round(chisq_test$statistic, 2), ",",
"p =", round(chisq_test$p.value, 3),
sep = " ")
}
# Auxiliary function for printing out wilcoxon signed-rank test statistics
report_wilcox_summary = function(wilcox_test) {
z_val = qnorm(wilcox_test$p.value / 2)
paste("z =", round(z_val, 2), ",",
"p =", round(wilcox_test$p.value, 3),
sep = " ")
}
# Generic plot theme for use in most graphs
individ_plot_theme = theme(
# titles
plot.title = element_text(face = "bold", size = 24),
axis.title.y = element_text(face = "bold", size = 24),
axis.title.x = element_text(face = "bold", size = 20),
legend.title = element_text(face = "bold", size = 16),
# axis text
axis.text.y = element_text(size = 16, face = "bold"),
axis.text.x = element_text(size = 16, face = "bold"),
# legend text
legend.text = element_text(size = 18, face = "bold"),
# backgrounds, lines
panel.background = element_blank(),
strip.background = element_blank(),
panel.grid = element_line(color = "gray"),
axis.line = element_line(color = "black"),
# positioning
legend.position = "bottom",
legend.key = element_rect(colour = "transparent", fill = "transparent")
)
# Line plot of prediction accuracy by condition for each trial
# (percent of participants in each condition who were correct in each trial)
# NB: this plot not included in cog sci submission
plot_prediction_summary = function(prediction_summary) {
prediction_summary %>%
filter(trial_index > 4) %>%
ggplot(aes(x = trial_index, y = accuracy, color = Condition)) +
geom_line() +
geom_hline(yintercept = 0.5, linetype = "dashed") +
labs(x = "Trial index", y = "Accuracy") +
# ggtitle("Prediction accuracy in each round") +
scale_color_viridis(discrete = T,
name = element_blank()) +
individ_plot_theme
}
# Bar chart of classification accuracy on binary generation judgment task by condition
plot_generation_judgments = function(generation_judgment_summary) {
generation_judgment_summary %>%
ggplot(aes(x = Condition, y = mean_accuracy,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", alpha = 0.5, width = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
geom_hline(yintercept = 0.5, linetype = "dashed", size = 1) +
# labs(x = "", y = "Mean classification accuracy") +
labs(x = "", y = "Percent Correct") +
# ggtitle("Accuracy on generation judgment task") +
ggtitle("Classification Accuracy") +
ylim(c(0, 1)) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Bar chart of rule generation accuracy for coded free response answers
# NB: this chart not included in cog sci submission
plot_generation_free_responses = function(generation_free_resp_summary) {
generation_free_resp_summary %>%
ggplot(aes(x = Condition, y = correct_generation_pct,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
labs(x = "", y = "Percent Correct") +
ggtitle("Free Response Accuracy") +
# ggtitle("Rule generation across conditions") +
ylim(c(0, 1)) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Bar chart of average evaluation ratings across conditions on rule evaluation task
plot_evaluation_results = function(evaluation_summary, comparison_set) {
evaluation_summary %>%
ggplot(aes(x = ruleset, y = mean_rating,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", position = position_dodge(preserve = "single"), width = 0.5, alpha = 0.5) +
geom_errorbar(
aes(ymin = ci_lower, ymax = ci_upper),
position = position_dodge(width = 0.5, preserve = "single"),
width = 0.2) +
labs(y = "Mean evaluation rating") +
# ggtitle("Evaluation across conditions") +
scale_x_discrete(name = element_blank()) +
#labels = comparison_set) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_y_continuous(breaks = seq(1, 7)) +
individ_plot_theme
}
# Bar chart of experiment completion time or avg. trial time
plot_time_data = function(time_summary, ylab, ymax, title) {
time_summary %>%
ggplot(aes(x = Condition, y = mean_task_time,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
ylim(0, ymax) +
labs(x = "", y = ylab) +
ggtitle(title) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank(),
plot.title = element_text(size = 32, face = "bold"))
}
# Bar chart of average memory accuracy across conditions
plot_memory_data = function(memory_summary) {
memory_summary %>%
ggplot(aes(x = Condition, y = mean_memory_accuracy,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
geom_hline(yintercept = 0.5, linetype = "dashed", size = 1) +
ylim(c(0, 1)) +
labs(x = "", y = "Mean memory accuracy") +
# ggtitle("Memory probe accuracy across conditions") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Bar chart of counts of feature references in coded explanations/descriptions, by condition
plot_coded_explanation_data = function(explanation_coded_summary) {
explanation_coded_summary %>%
ggplot(aes(x = measure, y = measure_mean,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", position = position_dodge(preserve = "single"),
width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = measure_mean - measure_se, ymax = measure_mean + measure_se),
position = position_dodge(width = 0.5, preserve = "single"),
width = 0.25) +
ggtitle("Explanation and description measures") +
labs(x = "", y = "Avg. count") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme
}
# Read in data
summary_data = bind_rows(read_summary_data(ROUND1_SUMMARY_DATA, TRUE),
read_summary_data(ROUND2_SUMMARY_DATA, FALSE))
trial_data = bind_rows(read_trial_data(ROUND1_TRIAL_DATA, TRUE),
read_trial_data(ROUND2_TRIAL_DATA, FALSE))
generation_free_resp_coded = read_coded_free_resp_data(GENERATION_RESP_DATA_CODED, summary_data)
generation_judgment_data = bind_rows(read_generation_judgment_data(ROUND1_GENERATION_JUDG_DATA, TRUE),
read_generation_judgment_data(ROUND2_GENERATION_JUDG_DATA, FALSE))
evaluation_data = bind_rows(read_evaluation_data(ROUND1_EVAL_DATA, TRUE),
read_evaluation_data(ROUND2_EVAL_DATA, FALSE))
memory_data = bind_rows(read_memory_data(ROUND1_MEMORY_DATA, TRUE),
read_memory_data(ROUND2_MEMORY_DATA, FALSE))
# Summarize data
prediction_summary = get_prediction_summary(trial_data)
generation_free_resp_summary = get_generation_free_response_summary(generation_free_resp_coded)
generation_judgment_subject_summary = get_generation_judgment_subj_summary(generation_judgment_data)
generation_judgment_summary = get_generation_judgment_summary(generation_judgment_subject_summary)
evaluation_summary = get_evaluation_summary(evaluation_data)
completion_time_summary = get_time_summary(summary_data)
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
trial_time_subject_summary = get_trial_time_subj_summary(trial_data)
trial_time_summary = get_trial_time_summary(trial_time_subject_summary)
memory_subject_summary = get_memory_subj_summary(memory_data)
memory_summary = get_memory_summary(memory_subject_summary)
# Coded explanation / description data
# TODO check that this lines up with subject numbers from regular data (seems we have fewer)
explanation_coded_data = read_coded_explanation_data(EXPLANATION_DATA_CODED)
explanation_coded_summary_subjects = get_explanation_coded_subj_summary(explanation_coded_data)
explanation_coded_summary = get_explanation_coded_summary(explanation_coded_summary_subjects)
plot_coded_explanation_data(explanation_coded_summary)
explanation_coded_summary_subjects
explanation_coded_summary_subjects %>% distinct(Subject)
tmp = read_summary_data(ROUND1_SUMMARY_DATA, TRUE)
tmp %>% distinct(subjID)
tmp2 = read_summary_data(ROUND2_SUMMARY_DATA, FALSE)
tmp2 %>% distinct(subjID)
