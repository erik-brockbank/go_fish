# axis text
axis.text.y = element_text(size = 16, face = "bold"),
axis.text.x = element_text(size = 16, face = "bold"),
# legend text
legend.text = element_text(size = 18, face = "bold"),
# backgrounds, lines
panel.background = element_blank(),
strip.background = element_blank(),
panel.grid = element_line(color = "gray"),
axis.line = element_line(color = "black"),
# positioning
legend.position = "bottom",
legend.key = element_rect(colour = "transparent", fill = "transparent")
)
# Line plot of prediction accuracy by condition for each trial
# (percent of participants in each condition who were correct in each trial)
# NB: this plot not included in cog sci submission
plot_prediction_summary = function(prediction_summary) {
prediction_summary %>%
filter(trial_index > 4) %>%
ggplot(aes(x = trial_index, y = accuracy, color = Condition)) +
geom_line() +
geom_hline(yintercept = 0.5, linetype = "dashed") +
labs(x = "Trial index", y = "Accuracy") +
# ggtitle("Prediction accuracy in each round") +
scale_color_viridis(discrete = T,
name = element_blank()) +
individ_plot_theme
}
# Bar chart of classification accuracy on binary generation judgment task by condition
plot_generation_judgments = function(generation_judgment_summary) {
generation_judgment_summary %>%
ggplot(aes(x = Condition, y = mean_accuracy,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", alpha = 0.5, width = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
geom_hline(yintercept = 0.5, linetype = "dashed", size = 1) +
# labs(x = "", y = "Mean classification accuracy") +
labs(x = "", y = "Percent Correct") +
# ggtitle("Accuracy on generation judgment task") +
ggtitle("Classification Accuracy") +
ylim(c(0, 1)) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Bar chart of rule generation accuracy for coded free response answers
# NB: this chart not included in cog sci submission
plot_generation_free_responses = function(generation_free_resp_summary) {
generation_free_resp_summary %>%
ggplot(aes(x = Condition, y = correct_generation_pct,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
labs(x = "", y = "Percent Correct") +
ggtitle("Free Response Accuracy") +
# ggtitle("Rule generation across conditions") +
ylim(c(0, 1)) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Bar chart of average evaluation ratings across conditions on rule evaluation task
plot_evaluation_results = function(evaluation_summary, comparison_set) {
evaluation_summary %>%
ggplot(aes(x = ruleset, y = mean_rating,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", position = position_dodge(preserve = "single"), width = 0.5, alpha = 0.5) +
geom_errorbar(
aes(ymin = ci_lower, ymax = ci_upper),
position = position_dodge(width = 0.5, preserve = "single"),
width = 0.2) +
labs(y = "Mean evaluation rating") +
# ggtitle("Evaluation across conditions") +
scale_x_discrete(name = element_blank()) +
#labels = comparison_set) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_y_continuous(breaks = seq(1, 7)) +
individ_plot_theme
}
# Bar chart of experiment completion time or avg. trial time
plot_time_data = function(time_summary, ylab, ymax, title) {
time_summary %>%
ggplot(aes(x = Condition, y = mean_task_time,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
ylim(0, ymax) +
labs(x = "", y = ylab) +
ggtitle(title) +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
#plot.title = element_text(size = 32, face = "bold"))
}
# Bar chart of average memory accuracy across conditions
plot_memory_data = function(memory_summary) {
memory_summary %>%
ggplot(aes(x = Condition, y = mean_memory_accuracy,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.25) +
geom_hline(yintercept = 0.5, linetype = "dashed", size = 1) +
ylim(c(0, 1)) +
labs(x = "", y = "Accuracy") +
ggtitle("Mean memory performance") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme +
theme(axis.text.x = element_blank())
}
# Bar chart of counts of feature references in coded explanations/descriptions, by condition
plot_coded_explanation_data = function(explanation_coded_summary) {
explanation_coded_summary %>%
ggplot(aes(x = measure, y = measure_mean,
color = Condition, fill = Condition)) +
geom_bar(stat = "identity", position = position_dodge(preserve = "single"),
width = 0.5, alpha = 0.5) +
geom_errorbar(aes(ymin = measure_mean - measure_se, ymax = measure_mean + measure_se),
position = position_dodge(width = 0.5, preserve = "single"),
width = 0.25) +
ggtitle("Explanation and description measures") +
labs(x = "", y = "Mean number of references") +
scale_color_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
scale_fill_viridis(discrete = T,
name = element_blank(),
# Change defaults to be blue/green instead of yellow/purple
begin = 0.25,
end = 0.75) +
individ_plot_theme
}
# Read in data
summary_data = bind_rows(read_summary_data(ROUND1_SUMMARY_DATA, TRUE),
read_summary_data(ROUND2_SUMMARY_DATA, FALSE))
trial_data = bind_rows(read_trial_data(ROUND1_TRIAL_DATA, TRUE),
read_trial_data(ROUND2_TRIAL_DATA, FALSE))
generation_free_resp_coded = read_coded_free_resp_data(GENERATION_RESP_DATA_CODED, summary_data)
generation_judgment_data = bind_rows(read_generation_judgment_data(ROUND1_GENERATION_JUDG_DATA, TRUE),
read_generation_judgment_data(ROUND2_GENERATION_JUDG_DATA, FALSE))
evaluation_data = bind_rows(read_evaluation_data(ROUND1_EVAL_DATA, TRUE),
read_evaluation_data(ROUND2_EVAL_DATA, FALSE))
memory_data = bind_rows(read_memory_data(ROUND1_MEMORY_DATA, TRUE),
read_memory_data(ROUND2_MEMORY_DATA, FALSE))
# Summarize data
prediction_summary = get_prediction_summary(trial_data)
generation_free_resp_summary = get_generation_free_response_summary(generation_free_resp_coded)
generation_judgment_subject_summary = get_generation_judgment_subj_summary(generation_judgment_data)
generation_judgment_summary = get_generation_judgment_summary(generation_judgment_subject_summary)
evaluation_summary = get_evaluation_summary(evaluation_data)
completion_time_summary = get_time_summary(summary_data)
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
trial_time_subject_summary = get_trial_time_subj_summary(trial_data)
trial_time_summary = get_trial_time_summary(trial_time_subject_summary)
memory_subject_summary = get_memory_subj_summary(memory_data)
memory_summary = get_memory_summary(memory_subject_summary)
# Coded explanation / description data
explanation_coded_data = read_coded_explanation_data(EXPLANATION_DATA_CODED)
explanation_coded_summary_subjects = get_explanation_coded_subj_summary(explanation_coded_data)
explanation_coded_summary = get_explanation_coded_summary(explanation_coded_summary_subjects)
# 1. Generation free response task
# NB: this plot not included in cog sci submission, but statistics are reported
generation_fr = plot_generation_free_responses(generation_free_resp_summary)
generation_props = generation_free_resp_coded %>%
group_by(Condition) %>%
summarize(success = sum(Revision),
total = n())
chisq_gen = prop.test(c(generation_props$success), c(generation_props$total))
report_chisq_summary(chisq_gen)
# 2. Generation judgment task: overall accuracy across conditions
generation_class = plot_generation_judgments(generation_judgment_summary)
t_gen = t.test(
generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Describe"],
generation_judgment_subject_summary$subj_accuracy[generation_judgment_subject_summary$Condition == "Explain"],
var.equal = T
)
report_t_summary(t_gen)
# 3. Generation judgment task: proportion of people who got 100% across conditions
# NB: similar Chi-sq test with raw count of people who were *coded as getting rule* is what we report above
# for the free response task so this is effectively a parallel analysis for people who got 100% on judgment task
generation_task_comparison = generation_free_resp_coded %>%
inner_join(generation_judgment_subject_summary, by = c("subjID", "Condition"))
# NB: we want table to be subj_accuracy == 1 but we do < 1 below to get count of == 1 as the *first* column
count_data = table(generation_task_comparison$Condition, generation_task_comparison$subj_accuracy < 1)
chisq.test(count_data) # NB: this is equivalent to prop.test(count_data)
# Plot results
generation_fr + generation_class
# 1. Evaluation of target rule across conditions
plot_evaluation_results(evaluation_summary, RULE_EVAL_LABELS)
t_eval = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe" & evaluation_data$is_target_rule == TRUE],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain" & evaluation_data$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_eval)
# 2. Wilcoxon signed-rank test showing that target rule is different from all other rules across both groups
eval_summary_other_rules = evaluation_data %>%
filter(is_target_rule == FALSE) %>%
group_by(is_target_rule, Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating))
eval_difference = evaluation_data %>%
group_by(subjID, Condition) %>%
filter(is_target_rule == TRUE) %>%
inner_join(., eval_summary_other_rules, by = "subjID") %>%
mutate(diff = input_rule_rating - mean_subj_rating) %>%
select(subjID, Condition.x, diff)
wil_exp = wilcox.test(eval_difference$diff[eval_difference$Condition.x == "Explain"], exact = F)
wil_des = wilcox.test(eval_difference$diff[eval_difference$Condition.x == "Describe"], exact = F)
report_wilcox_summary(wil_exp) # Explainers
report_wilcox_summary(wil_des) # Describers
# 3.1 Evaluation of distractor rule across conditions
t_eval_comp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == DISTRACTOR_RULE],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_eval_comp)
# 3.2 Comparison of distractor rule and target rule across conditions
t_eval_target_dist_exp = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Explain"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
t_eval_target_dist_desc = t.test(
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
evaluation_data$input_rule_rating[evaluation_data$Condition == "Describe"
& evaluation_data$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_eval_target_dist_exp) # Explainers
report_t_summary(t_eval_target_dist_desc) # Describers
# Test whether this difference is significant with ANOVA
eval_data_distractor_target = evaluation_data %>%
filter(is_target_rule == T | rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish.")
# sanity check
unique(eval_data_distractor_target$rule_text)
table(eval_data_distractor_target$subjID)
# check for significant interaction between condition and target rule v. distractor
interaction_test = aov(data = eval_data_distractor_target, input_rule_rating ~ Condition*is_target_rule)
summary(interaction_test)
# getting DF for reporting F statistics in Anova above
unique(eval_data_distractor_target$subjID)
# 4.1 Evaluation of target rule across conditions (this matches 1 above)
subset_participants = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
filter(Revision == 0) %>%
# filter(Revision == 1) %>%
inner_join(., evaluation_data, by = c("subjID", "Condition"))
# Sanity check the join above
table(subset_participants$subjID) # 6 (number of eval rows) for each subj ID
unique(subset_participants$subjID) # 56 for incorrect rule gen, 30 for correct
sum(generation_free_resp_coded$Revision) # 30 for incorrect rule gen: this is equal to 86 (total) - number of unique participants above or equal to number of unique part. above
# Plot data
eval_summary_subset = get_evaluation_summary(subset_participants)
# NB: this plot not included in manuscript
plot_evaluation_results(eval_summary_subset, RULE_EVAL_LABELS)
# Analysis: compare ratings of target rule
t_subset_target = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$is_target_rule == TRUE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_subset_target)
# 4.2. Wilcoxon signed-rank test showing that target rule is different from all other rules *among participants who didn't get the correct rule*
eval_summary_other_rules_subset = subset_participants %>%
filter(is_target_rule == FALSE) %>% # for target rule, summarize across participants
group_by(is_target_rule, Condition, subjID) %>%
summarize(mean_subj_rating = mean(input_rule_rating))
eval_difference_subset = subset_participants %>%
group_by(subjID, Condition) %>%
filter(is_target_rule == TRUE) %>%
inner_join(., eval_summary_other_rules_subset, by = "subjID") %>%
mutate(diff = input_rule_rating - mean_subj_rating) %>%
select(subjID, Condition.x, diff)
wil_exp_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Explain"], exact = F)
wil_des_subset = wilcox.test(eval_difference_subset$diff[eval_difference_subset$Condition.x == "Describe"], exact = F)
report_wilcox_summary(wil_exp_subset) # Explainers
report_wilcox_summary(wil_des_subset) # Describers
# 4.3 Evaluation of distractor rule across conditions (this matches 3.1 above)
t_subset_dist = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == DISTRACTOR_RULE],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == DISTRACTOR_RULE],
var.equal = T
)
report_t_summary(t_subset_dist)
# 4.3 Comparison of distractor rule and target rule across conditions (this matches 3.2 above)
t_subset_target_dist_exp = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
subset_participants$input_rule_rating[subset_participants$Condition == "Explain"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
t_subset_target_dist_desc = t.test(
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish."],
subset_participants$input_rule_rating[subset_participants$Condition == "Describe"
& subset_participants$is_target_rule == TRUE],
var.equal = T
)
report_t_summary(t_subset_target_dist_exp) # Explainers
report_t_summary(t_subset_target_dist_desc) # Describers
# Test whether this difference is significant with ANOVA
subset_participants_distractor_target = subset_participants %>%
filter(is_target_rule == T | rule_text == "If a lure combination has a yellow shape or a diamond on the bottom, it will catch fish.")
# sanity check
unique(subset_participants_distractor_target$rule_text)
table(subset_participants_distractor_target$subjID)
# check for significant interaction between condition and target rule v. distractor
interaction_test = aov(data = subset_participants_distractor_target, input_rule_rating ~ Condition*is_target_rule)
summary(interaction_test)
# getting DF for reporting F statistics in Anova above
unique(subset_participants_distractor_target$subjID)
# 1. Memory performance compared to chance
# NB: not doing binomial test here because we are looking at accuracy percentages for N subjects
t_mem_chance_exp = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
mu = 0.5,
equal.var = T)
t_mem_chance_desc = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
mu = 0.5,
equal.var = T)
report_t_summary(t_mem_chance_exp)
report_t_summary(t_mem_chance_desc)
# 2. Memory accuracy across conditions
mem = plot_memory_data(memory_summary) # This plot is combined with time on task plots further down
t_mem = t.test(memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Describe"],
memory_subject_summary$subj_accuracy[memory_subject_summary$Condition == "Explain"],
var.equal = T)
report_t_summary(t_mem)
# 3. Memory performance comparing participants who did and didn't get the correct rule
memory_hypothesis_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., memory_subject_summary, by = c("subjID"))
t_mem_correct = t.test(memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 0],
memory_hypothesis_join$subj_accuracy[memory_hypothesis_join$Revision == 1],
var.equal = T)
report_t_summary(t_mem_correct)
# 1. Overall time on task across conditions
# This plot is combined with memory and other time on task plots further down
time_on_task = plot_time_data(completion_time_summary, ylab = "Seconds", ymax = 1000, title = "Mean time on experiment")
t_time = t.test(summary_data$experiment_completion_time[summary_data$Condition == "Describe"],
summary_data$experiment_completion_time[summary_data$Condition == "Explain"],
var.equal = T)
report_t_summary(t_time) # Means are seconds on task
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
unique(trial_time_subject_summary$subjID[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Round1[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Condition[is.na(trial_time_subject_summary$mean_trial_completion)])
# This plot combined with earlier memory and time on task plots further down
time_on_trials = plot_time_data(trial_time_summary, ylab = "Seconds", ymax = 80, title = "Mean time on trials")
t_trials = t.test(trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Describe" &
trial_time_subject_summary$Round1 == FALSE],
trial_time_subject_summary$mean_trial_completion[trial_time_subject_summary$Condition == "Explain" &
trial_time_subject_summary$Round1 == FALSE],
var.equal = T)
report_t_summary(t_trials) # Means are seconds on trials
# Plot graphs from 1. and 2. above side by side with patchwork
# time_on_task + time_on_trials
mem + time_on_task + time_on_trials +
plot_annotation(tag_levels = 'A') &
theme(plot.tag = element_text(size = 20, face = "bold"))
# 3. Overall time on task comparing participants who did and didn't get the correct rule
task_time_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., summary_data, by = c("subjID", "Condition")) %>%
select(subjID, Condition, Revision, experiment_completion_time)
t_task_time_correct = t.test(task_time_join$experiment_completion_time[task_time_join$Revision == 0],
task_time_join$experiment_completion_time[task_time_join$Revision == 1],
var.equal = T)
report_t_summary(t_task_time_correct) # Means are seconds on task
# NB: 17 initial round1 subjects have incomplete trial time data; impacts trial_time_subject_summary and trial_time_summary
unique(trial_time_subject_summary$subjID[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Round1[is.na(trial_time_subject_summary$mean_trial_completion)])
table(trial_time_subject_summary$Condition[is.na(trial_time_subject_summary$mean_trial_completion)])
trial_time_join = generation_free_resp_coded %>%
select(subjID, Condition, Revision) %>%
inner_join(., trial_time_subject_summary, by = c("subjID", "Condition")) %>%
select(subjID, Condition, Revision, mean_trial_completion)
t_trial_time_correct = t.test(trial_time_join$mean_trial_completion[trial_time_join$Revision == 0],
trial_time_join$mean_trial_completion[trial_time_join$Revision == 1],
var.equal = T)
report_t_summary(t_trial_time_correct) # Means are seconds on trials
# sanity checks
glimpse(explanation_coded_data)
glimpse(explanation_coded_summary_subjects)
glimpse(explanation_coded_summary)
unique(explanation_coded_data$Subject)
# Plot results
plot_coded_explanation_data(explanation_coded_summary)
# Analysis (copied from Williams & Lombrozo, 2010)
concrete_data = explanation_coded_summary_subjects %>%
filter(measure %in% c("shape_concrete_total", "color_concrete_total", "purple_dot_concrete_total"))
anova_concrete = aov(data = concrete_data, subject_total ~ Condition + measure)
summary(anova_concrete)
abstract_data = explanation_coded_summary_subjects %>%
filter(measure %in% c("shape_abstract_total", "color_abstract_total", "purple_dot_abstract_total"))
anova_abstract = aov(data = abstract_data, subject_total ~ Condition + measure)
summary(anova_abstract)
# Check mechanism effect
t_mech = t.test(explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "mechanism_total" &
explanation_coded_summary_subjects$Condition == "Explain"],
explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "mechanism_total" &
explanation_coded_summary_subjects$Condition == "Describe"],
var.equal = T)
report_t_summary(t_mech) # Means are avg. number of references
# Analysis (copied from Williams & Lombrozo, 2010)
# First, do abstract feature references show a main effect of condition
abstract_data = explanation_coded_summary_subjects %>%
filter(measure %in% c("shape_abstract_total", "color_abstract_total", "purple_dot_abstract_total"))
anova_abstract = aov(data = abstract_data, subject_total ~ Condition + measure)
summary(anova_abstract)
# Do concrete feature references show a main effect of condition?
# i.e. do control pariticipants make more concrete references?
concrete_data = explanation_coded_summary_subjects %>%
filter(measure %in% c("shape_concrete_total", "color_concrete_total", "purple_dot_concrete_total"))
anova_concrete = aov(data = concrete_data, subject_total ~ Condition + measure)
summary(anova_concrete)
# Shape
t_shape_abs = t.test(explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "shape_abstract_total" &
explanation_coded_summary_subjects$Condition == "Explain"],
explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "shape_abstract_total" &
explanation_coded_summary_subjects$Condition == "Describe"],
var.equal = T)
report_t_summary(t_shape_abs) # Means are avg. number of references
t_shape_conc = t.test(explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "shape_concrete_total" &
explanation_coded_summary_subjects$Condition == "Explain"],
explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "shape_concrete_total" &
explanation_coded_summary_subjects$Condition == "Describe"],
var.equal = T)
report_t_summary(t_shape_conc) # Means are avg. number of references
# Color
t_color_abs = t.test(explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "color_abstract_total" &
explanation_coded_summary_subjects$Condition == "Explain"],
explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "color_abstract_total" &
explanation_coded_summary_subjects$Condition == "Describe"],
var.equal = T)
report_t_summary(t_color_abs) # Means are avg. number of references
t_color_conc = t.test(explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "color_concrete_total" &
explanation_coded_summary_subjects$Condition == "Explain"],
explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "color_concrete_total" &
explanation_coded_summary_subjects$Condition == "Describe"],
var.equal = T)
report_t_summary(t_color_conc) # Means are avg. number of references
# Purple dot
t_dot_abs = t.test(explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "purple_dot_abstract_total" &
explanation_coded_summary_subjects$Condition == "Explain"],
explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "purple_dot_abstract_total" &
explanation_coded_summary_subjects$Condition == "Describe"],
var.equal = T)
report_t_summary(t_dot_abs) # Means are avg. number of references
t_dot_conc = t.test(explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "purple_dot_concrete_total" &
explanation_coded_summary_subjects$Condition == "Explain"],
explanation_coded_summary_subjects$subject_total[explanation_coded_summary_subjects$measure == "purple_dot_concrete_total" &
explanation_coded_summary_subjects$Condition == "Describe"],
var.equal = T)
report_t_summary(t_dot_conc) # Means are avg. number of references
2055 + 280 + 271 + 323 + 613 + 406 + 394 + 904 + 412 + 983
